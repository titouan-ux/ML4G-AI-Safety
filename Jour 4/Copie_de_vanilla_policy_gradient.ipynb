{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOxaIn9wPVnf"
      },
      "source": [
        "## Vanilla Policy Optimisation\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d5/vanilla_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "Preliminary questions:\n",
        "- Run the script with the defaults parameters on the terminal\n",
        "- Explain from torch.distributions.categorical import Categorical\n",
        "- google gym python, why is it useful?\n",
        "- Policy gradient is model based or model free?\n",
        "- Is policy gradient on-policy or off-policy?\n",
        "\n",
        "Read all the code, then:\n",
        "- Use https://github.com/patrick-kidger/torchtyping to type the functions get_policy, get_action and compute_loss\n",
        "- Type completely the whole code.\n",
        "- Use from typeguard import typechecked and the @typechecked decorator to check the previous question.\n",
        "- Answer the questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9S_h2PbPVnu",
        "outputId": "4896eec5-3010-4970-cf93-c7070f41abb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtyping in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchtyping) (1.12.1+cu113)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from torchtyping) (2.13.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->torchtyping) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtyping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W9Ltvh0OPVn1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "from torchtyping import TensorType, patch_typeguard\n",
        "from typeguard import typechecked\n",
        "\n",
        "patch_typeguard()  # use before @typechecked\n",
        "\n",
        "\n",
        "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
        "    # Build a feedforward neural network.\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    # What does * mean here? Search for unpacking in python\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
        "          epochs=50, batch_size=5000, render=False):\n",
        "\n",
        "    # make environment, check spaces, get obs / act dims\n",
        "    env = gym.make(env_name)\n",
        "    assert isinstance(env.observation_space, Box), \\\n",
        "        \"This example only works for envs with continuous state spaces.\"\n",
        "    assert isinstance(env.action_space, Discrete), \\\n",
        "        \"This example only works for envs with discrete action spaces.\"\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_acts = env.action_space.n\n",
        "\n",
        "    # Core of policy network\n",
        "    # What should be the sizes of the layers of the policy network?\n",
        "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
        "\n",
        "    # make function to compute action distribution\n",
        "    # What is the shape of obs? \n",
        "    @typechecked\n",
        "    def get_policy(obs:TensorType[..., obs_dim])->TensorType[..., n_acts]:\n",
        "        # Warning: obs has not always the same shape.\n",
        "        logits = logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    # make action selection function (outputs int actions, sampled from policy)\n",
        "    # What is the shape of obs?\n",
        "    @typechecked\n",
        "    def get_action(obs : TensorType[..., obs_dim]):  \n",
        "        return get_policy(obs).sample().item()\n",
        "\n",
        "    # make loss function whose gradient, for the right data, is policy gradient\n",
        "    # What does the weights parameter represents here?\n",
        "    # What is the shape of obs?\n",
        "    @typechecked\n",
        "    def compute_loss(obs : TensorType[..., obs_dim] , act , weights: torch.tensor()):  \n",
        "        logp = get_policy(obs).log_prob(act)\n",
        "        return -(logp * weights).mean()\n",
        "\n",
        "    # make optimizer\n",
        "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
        "\n",
        "    # for training policy\n",
        "    def train_one_epoch():\n",
        "        # make some empty lists for logging.\n",
        "        batch_obs = []          # for observations\n",
        "        batch_acts = []         # for actions\n",
        "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
        "        batch_rets = []         # for measuring episode returns # What is the return?\n",
        "        batch_lens = []         # for measuring episode lengths\n",
        "\n",
        "        # reset episode-specific variables\n",
        "        obs = env.reset()       # first obs comes from starting distribution \n",
        "        done = False            # signal from environment that episode is over\n",
        "        ep_rews = []            # list for rewards accrued throughout ep\n",
        "\n",
        "        # render first episode of each epoch\n",
        "        finished_rendering_this_epoch = False\n",
        "\n",
        "        # collect experience by acting in the environment with current policy\n",
        "        while True:\n",
        "\n",
        "            # rendering\n",
        "            if (not finished_rendering_this_epoch) and render:\n",
        "                env.render()\n",
        "\n",
        "            # save obs\n",
        "            batch_obs.append(obs.copy())\n",
        "\n",
        "            # act in the environment\n",
        "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
        "            obs, rew, done, _ = env.step(act)\n",
        "\n",
        "            # save action, reward\n",
        "            batch_acts.append(act)\n",
        "            ep_rews.append(rew)\n",
        "\n",
        "            if done:\n",
        "                # if episode is over, record info about episode\n",
        "                # Is the reward discounted?\n",
        "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
        "                batch_rets.append(ep_ret)\n",
        "                batch_lens.append(ep_len)\n",
        "\n",
        "                # the weight for each logprob(a|s) is R(tau)\n",
        "                # Why do we use a constant vector here?\n",
        "                batch_weights += [ep_ret] * ep_len          \n",
        "\n",
        "                # reset episode-specific variables\n",
        "                obs, done, ep_rews = env.reset(), False, []\n",
        "\n",
        "                # won't render again this epoch\n",
        "                finished_rendering_this_epoch = True\n",
        "\n",
        "                # end experience loop if we have enough of it\n",
        "                if len(batch_obs) > batch_size:\n",
        "                    break\n",
        "\n",
        "        # take a single policy gradient update step\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
        "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
        "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
        "                                  )\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        return batch_loss, batch_rets, batch_lens\n",
        "\n",
        "    # training loop\n",
        "    for i in range(epochs):\n",
        "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
        "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%(i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBYh7kx9PVpU",
        "outputId": "1c486ffd-c852-4c24-a27b-68e9a3b39b30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/typeguard/__init__.py:1016: UserWarning: no type annotations present -- not typechecking __main__.train.<locals>.compute_loss\n",
            "  warn('no type annotations present -- not typechecking {}'.format(function_name(func)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.04258865 -0.00495045 -0.02861075  0.00633207]\n",
            "epoch:   0 \t loss: 19.131 \t return: 18.000 \t ep_len: 18.000\n",
            "[-0.03541778 -0.04465922 -0.02410383 -0.01964713]\n",
            "epoch:   1 \t loss: 19.934 \t return: 25.000 \t ep_len: 25.000\n",
            "[ 0.04988953 -0.0241212  -0.00348926 -0.02824957]\n",
            "epoch:   2 \t loss: 11.667 \t return: 16.250 \t ep_len: 16.250\n",
            "[ 0.02765383 -0.01083768  0.01397943  0.0404648 ]\n",
            "epoch:   3 \t loss: 10.526 \t return: 15.000 \t ep_len: 15.000\n",
            "[ 0.04717234 -0.02667601  0.01827999 -0.04870812]\n",
            "epoch:   4 \t loss: 10.019 \t return: 14.250 \t ep_len: 14.250\n",
            "[ 0.03199461 -0.00194721 -0.04808279 -0.03889005]\n",
            "epoch:   5 \t loss: 16.431 \t return: 21.667 \t ep_len: 21.667\n",
            "[-0.00515325 -0.03407977  0.03670634  0.04267851]\n",
            "epoch:   6 \t loss: 21.823 \t return: 31.500 \t ep_len: 31.500\n",
            "[-0.00410539  0.02881444  0.00626996 -0.04292123]\n",
            "epoch:   7 \t loss: 20.437 \t return: 28.333 \t ep_len: 28.333\n",
            "[-0.0382729   0.00152748  0.01897391  0.02371486]\n",
            "epoch:   8 \t loss: 9.230 \t return: 13.000 \t ep_len: 13.000\n",
            "[-0.00642318  0.01949831 -0.00800853  0.04544104]\n",
            "epoch:   9 \t loss: 16.075 \t return: 21.667 \t ep_len: 21.667\n",
            "[ 0.03292392  0.00123113 -0.02464577  0.02060525]\n",
            "epoch:  10 \t loss: 18.880 \t return: 26.667 \t ep_len: 26.667\n",
            "[-0.04182529 -0.00683008 -0.01177485  0.03533098]\n",
            "epoch:  11 \t loss: 13.965 \t return: 19.333 \t ep_len: 19.333\n",
            "[ 0.03236166 -0.03885964  0.00796869 -0.01800958]\n",
            "epoch:  12 \t loss: 11.291 \t return: 15.250 \t ep_len: 15.250\n",
            "[-0.01320736 -0.03073263  0.03881003  0.04779045]\n",
            "epoch:  13 \t loss: 32.545 \t return: 39.000 \t ep_len: 39.000\n",
            "[ 0.00636337  0.00390879 -0.03701839 -0.00465806]\n",
            "epoch:  14 \t loss: 11.290 \t return: 16.250 \t ep_len: 16.250\n",
            "[-0.00672039 -0.03321356 -0.02332116 -0.03633953]\n",
            "epoch:  15 \t loss: 33.370 \t return: 46.000 \t ep_len: 46.000\n",
            "[ 0.03485949 -0.02466514  0.03548179  0.01317516]\n",
            "epoch:  16 \t loss: 14.459 \t return: 19.500 \t ep_len: 19.500\n",
            "[ 0.00821934 -0.02402334 -0.03114307 -0.04262623]\n",
            "epoch:  17 \t loss: 15.350 \t return: 20.000 \t ep_len: 20.000\n",
            "[0.01623362 0.03302958 0.03396426 0.03619667]\n",
            "epoch:  18 \t loss: 14.328 \t return: 20.333 \t ep_len: 20.333\n",
            "[ 0.01851292  0.04196873  0.00240067 -0.04388397]\n",
            "epoch:  19 \t loss: 20.508 \t return: 26.667 \t ep_len: 26.667\n",
            "[ 0.01472717  0.0256978  -0.01960004  0.00166171]\n",
            "epoch:  20 \t loss: 21.589 \t return: 27.667 \t ep_len: 27.667\n",
            "[ 0.01958486 -0.03602767 -0.02586328  0.04704283]\n",
            "epoch:  21 \t loss: 43.234 \t return: 63.000 \t ep_len: 63.000\n",
            "[-0.02365889 -0.01349723  0.00285596  0.00285566]\n",
            "epoch:  22 \t loss: 17.266 \t return: 26.000 \t ep_len: 26.000\n",
            "[ 0.00737733  0.0238688   0.03512197 -0.01195529]\n",
            "epoch:  23 \t loss: 16.339 \t return: 19.667 \t ep_len: 19.667\n",
            "[ 0.01514689 -0.02958957 -0.01243182  0.00274229]\n",
            "epoch:  24 \t loss: 37.664 \t return: 56.000 \t ep_len: 56.000\n",
            "[-0.04184416  0.00667893  0.04333813  0.01289021]\n",
            "epoch:  25 \t loss: 22.560 \t return: 29.667 \t ep_len: 29.667\n",
            "[-0.01855654 -0.02227491  0.01623346 -0.0207375 ]\n",
            "epoch:  26 \t loss: 43.428 \t return: 65.000 \t ep_len: 65.000\n",
            "[-0.03927113 -0.03113622 -0.03485939 -0.02742877]\n",
            "epoch:  27 \t loss: 14.155 \t return: 20.333 \t ep_len: 20.333\n",
            "[ 0.0352832   0.04724727  0.02131251 -0.00494889]\n",
            "epoch:  28 \t loss: 16.267 \t return: 24.000 \t ep_len: 24.000\n",
            "[0.04149215 0.0194362  0.04465582 0.03346203]\n",
            "epoch:  29 \t loss: 18.066 \t return: 26.500 \t ep_len: 26.500\n",
            "[ 0.0030762  -0.04100462 -0.00381861 -0.0441535 ]\n",
            "epoch:  30 \t loss: 48.889 \t return: 74.000 \t ep_len: 74.000\n",
            "[-0.03452826 -0.02607305 -0.00497286  0.02039632]\n",
            "epoch:  31 \t loss: 24.280 \t return: 31.000 \t ep_len: 31.000\n",
            "[-0.04654942 -0.00281453 -0.04048605  0.03019265]\n",
            "epoch:  32 \t loss: 33.927 \t return: 38.000 \t ep_len: 38.000\n",
            "[0.01622746 0.04434806 0.01236665 0.04861394]\n",
            "epoch:  33 \t loss: 48.347 \t return: 71.000 \t ep_len: 71.000\n",
            "[ 0.00561416 -0.0175826   0.04235609 -0.00273167]\n",
            "epoch:  34 \t loss: 59.259 \t return: 91.000 \t ep_len: 91.000\n",
            "[ 0.01785488 -0.03221677  0.02143044  0.03280817]\n",
            "epoch:  35 \t loss: 17.343 \t return: 27.000 \t ep_len: 27.000\n",
            "[ 0.00598674  0.0390873   0.00273169 -0.02811697]\n",
            "epoch:  36 \t loss: 23.203 \t return: 35.000 \t ep_len: 35.000\n",
            "[-0.01924261  0.00424996 -0.01231026 -0.03384938]\n",
            "epoch:  37 \t loss: 21.478 \t return: 34.000 \t ep_len: 34.000\n",
            "[ 0.0177524  -0.00203246 -0.04531842  0.03352564]\n",
            "epoch:  38 \t loss: 27.792 \t return: 39.000 \t ep_len: 39.000\n",
            "[ 0.02468531 -0.00456948  0.0061554   0.04097901]\n",
            "epoch:  39 \t loss: 21.898 \t return: 28.000 \t ep_len: 28.000\n",
            "[-0.00019848  0.04829043 -0.01687704  0.01511724]\n",
            "epoch:  40 \t loss: 19.506 \t return: 27.667 \t ep_len: 27.667\n",
            "[-0.03322899 -0.03762324  0.04002795 -0.04820013]\n",
            "epoch:  41 \t loss: 31.532 \t return: 38.333 \t ep_len: 38.333\n",
            "[-0.0165187  -0.02711819  0.0036012   0.02959665]\n",
            "epoch:  42 \t loss: 17.343 \t return: 27.000 \t ep_len: 27.000\n",
            "[ 0.0349733   0.01477273 -0.04420736  0.01201862]\n",
            "epoch:  43 \t loss: 27.720 \t return: 31.500 \t ep_len: 31.500\n",
            "[-0.00718637  0.03612329 -0.03780161 -0.03962459]\n",
            "epoch:  44 \t loss: 23.723 \t return: 33.500 \t ep_len: 33.500\n",
            "[-0.01108511 -0.04859154  0.02904398 -0.00310176]\n",
            "epoch:  45 \t loss: 43.770 \t return: 51.500 \t ep_len: 51.500\n",
            "[ 0.03639581 -0.03956234 -0.03601541  0.00448448]\n",
            "epoch:  46 \t loss: 25.964 \t return: 35.000 \t ep_len: 35.000\n",
            "[ 0.03567912 -0.02003441  0.01456827 -0.01636116]\n",
            "epoch:  47 \t loss: 41.639 \t return: 44.667 \t ep_len: 44.667\n",
            "[-0.0455147  -0.00719741 -0.03093103  0.02386337]\n",
            "epoch:  48 \t loss: 15.656 \t return: 25.000 \t ep_len: 25.000\n",
            "[-0.02911413 -0.0410969  -0.00642735 -0.03539994]\n",
            "epoch:  49 \t loss: 22.777 \t return: 35.000 \t ep_len: 35.000\n"
          ]
        }
      ],
      "source": [
        "train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
        "          epochs=50, batch_size=50, render=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-mKdrcrPVpV"
      },
      "outputs": [],
      "source": [
        "Original algo here: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/vpg/vpg.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6809ed8a35337f3a27b071f62b35d4692d9130abc381b82b2f84ea8d71c60dfb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
